{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788188ee",
   "metadata": {},
   "source": [
    "### - Effect of scaling Example - \n",
    "### Scaling by squere root of dimension during computation of Queries and Keys Matrices before applying softmax and calculating attention weights for causal attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830b3eb",
   "metadata": {},
   "source": [
    "## 1. stability in learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f5668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result without without multiplying: tensor([0.1669, 0.1119, 0.1013, 0.2039, 0.1669, 0.2490])\n",
      "Result after multiplying: tensor([0.0443, 0.0027, 0.0013, 0.1795, 0.0443, 0.7279])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([0.3, -0.1, -0.2, 0.5, 0.3, 0.7])\n",
    "scaled_tensor = tensor * 7\n",
    "\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "\n",
    "print(f\"Result without without multiplying: {softmax_result}\")\n",
    "print(f\"Result after multiplying: {softmax_scaled_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e116de2",
   "metadata": {},
   "source": [
    "#### comparing the results shows how the output after scaling becomes peaky as the last value is unproportional high in comparison to the other values, in order to avoid such sharp softmax distribution the scaling by the square root of the dimensions in performed, leads to the model becoming over confident in the one key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed708f",
   "metadata": {},
   "source": [
    "## 2. to stabelize the variance of the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bcb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling for dimension = 5: 4.821266469703955\n",
      "Variance after scaling for dimension = 5: 0.9642532939407908\n",
      "Variance before scaling for dimension = 30: 20.403357325660153\n",
      "Variance after scaling for dimension = 30: 1.0201678662830074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_variance(dim, trails=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    for _ in range(trails):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        scaled_dot_product = dot_product / dim**0.5\n",
    "        scaled_dot_product = dot_product / torch.sqrt(torch.tensor(dim))\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "variance_before_30, variance_after_30 = compute_variance(20)\n",
    "\n",
    "print(f\"Variance before scaling for dimension = 5: {variance_before_5}\")\n",
    "print(f\"Variance after scaling for dimension = 5: {variance_after_5}\")\n",
    "\n",
    "print(f\"Variance before scaling for dimension = 30: {variance_before_30}\")\n",
    "print(f\"Variance after scaling for dimension = 30: {variance_after_30}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482e138",
   "metadata": {},
   "source": [
    "### as dimensions of Trainable Weight Matrices increase the variance also becomes larger which might cause unstable learning, after scaling by the square root of the dimensions the variance always stays close to 1 which ensures stability in learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_built_steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

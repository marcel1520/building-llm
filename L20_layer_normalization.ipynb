{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318fc163",
   "metadata": {},
   "source": [
    "# Lecture 20: Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a58ed802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "8f4f95ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch:\n",
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
      "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])\n",
      "\n",
      "Layer Output:\n",
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "emb_dim = batch_example.shape[-1]\n",
    "hidden_layers = 6\n",
    "layer = nn.Sequential(nn.Linear(emb_dim, hidden_layers), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "\n",
    "print(f\"Input Batch:\\n{batch_example}\\n\")\n",
    "print(f\"Layer Output:\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "0d9ffd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Mean Value:\n",
      "tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "Output Variance:\n",
      "tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(f\"Output Mean Value:\\n{mean}\\n\")\n",
    "print(f\"Output Variance:\\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "50192df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Output normalized:\n",
      "tensor([[-0.8603, -0.8603,  0.5869,  1.4698,  0.5242, -0.8603],\n",
      "        [-0.7450, -0.7450, -0.0102,  0.4844,  1.7608, -0.7450]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "\n",
      "Mean Value of Normalized Output (Sci Mode set to False):\n",
      "tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "Variance Value of normalized Output:\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "out_normalized = (out - mean) / torch.sqrt(var)\n",
    "mean = out_normalized.mean(dim=-1, keepdim=True)\n",
    "var = out_normalized.var(dim=-1, keepdim=True)\n",
    "print(f\"Layer Output normalized:\\n{out_normalized}\\n\")\n",
    "print(f\"Mean Value of Normalized Output (Sci Mode set to False):\\n{mean}\\n\")\n",
    "print(f\"Variance Value of normalized Output:\\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "b6f32e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "3b5284d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Batch:\n",
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
      "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])\n",
      "\n",
      "Mean Value:\n",
      "tensor([[0.3654],\n",
      "        [0.4032]])\n",
      "\n",
      "Variance Value:\n",
      "tensor([[0.0460],\n",
      "        [0.1057]])\n",
      "\n",
      "Output normalized:\n",
      "tensor([[-0.3229,  0.7049, -0.5302,  1.5069, -1.3587],\n",
      "        [ 1.4247, -0.8199, -0.9248, -0.6739,  0.9940]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Mean Value:\n",
      "tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "Variance Value:\n",
      "tensor([[0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(emb_dim=emb_dim)\n",
    "output_normalized = layer_norm.forward(batch_example)\n",
    "print(f\"Example Batch:\\n{batch_example}\\n\")\n",
    "print(f\"Mean Value:\\n{batch_example.mean(dim=-1, keepdim=True)}\\n\\nVariance Value:\\n{batch_example.var(dim=-1, unbiased=False, keepdim=True)}\\n\")\n",
    "print(f\"Output normalized:\\n{output_normalized}\\n\")\n",
    "print(f\"Mean Value:\\n{output_normalized.mean(dim=-1, keepdim=True)}\\n\")\n",
    "print(f\"Variance Value:\\n{output_normalized.var(dim=-1, unbiased=False, keepdim=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "a8d91a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row sum applying Softmax Normalization: 1.00\n",
      "Row sum applying Layer Normalization: 0.00\n",
      "\n",
      "Example Batch Softmax normalized:\n",
      "tensor([[0.1511, 0.1768, 0.2551, 0.2218, 0.1951],\n",
      "        [0.2221, 0.2242, 0.2417, 0.1868, 0.1252],\n",
      "        [0.1455, 0.3037, 0.2042, 0.1322, 0.2145]])\n",
      "Mean Values:\n",
      "tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]])\n",
      "Variance Values:\n",
      "tensor([[0.0013],\n",
      "        [0.0017],\n",
      "        [0.0037]])\n",
      "\n",
      "Example Batch Layer normalization:\n",
      "tensor([[-1.4612, -0.5941,  1.4379,  0.6639, -0.0465],\n",
      "        [ 0.5510,  0.5900,  0.9076, -0.1802, -1.8685],\n",
      "        [-0.9192,  1.5546,  0.2206, -1.2413,  0.3852]], grad_fn=<AddBackward0>)\n",
      "Mean Values:\n",
      "tensor([[     0.0000],\n",
      "        [     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance Values:\n",
      "tensor([[0.9997],\n",
      "        [0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "batch_example1 = torch.rand(3, 5)\n",
    "\n",
    "output_normalized_sm = torch.softmax(batch_example1, dim=-1)\n",
    "output_normalized_nl = layer_norm.forward(batch_example1)\n",
    "\n",
    "sum_sm = sum(output_normalized_sm[1])\n",
    "sum_nl = sum(output_normalized_nl[1])\n",
    "\n",
    "on_sm_mean = output_normalized_sm.mean(dim=-1, keepdim=True)\n",
    "on_nl_mean = output_normalized_nl.mean(dim=-1, keepdim=True)\n",
    "\n",
    "on_sm_var = output_normalized_sm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "on_nl_var = output_normalized_nl.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(f\"Row sum applying Softmax Normalization: {sum_sm:.2f}\\nRow sum applying Layer Normalization: {sum_nl:.2f}\\n\")\n",
    "print(f\"Example Batch Softmax normalized:\\n{output_normalized_sm}\\nMean Values:\\n{on_sm_mean}\\nVariance Values:\\n{on_sm_var}\\n\")\n",
    "print(f\"Example Batch Layer normalization:\\n{output_normalized_nl}\\nMean Values:\\n{on_nl_mean}\\nVariance Values:\\n{on_nl_var}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_built_steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

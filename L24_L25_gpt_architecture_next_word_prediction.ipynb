{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e052d0e",
   "metadata": {},
   "source": [
    "# Lecture 24: GPT Architecture --> refer to model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951f988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e708a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f75051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch shape:\n",
      "torch.Size([2, 4])\n",
      "Output:\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Output shape:\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "from model_with_feature_classes import GPTModel\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.tensor([[6109, 3626, 6100, 345],\n",
    "                      [6109, 1110, 6622, 257]])\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "\n",
    "print(f\"Input batch:\\n{batch}\\nInput batch shape:\\n{batch.shape}\")\n",
    "print(f\"Output:\\n{out}\\nOutput shape:\\n{out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ff0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Token Embedding Layer: torch.Size([50257, 768])\n",
      "Shape Output Layer: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape Token Embedding Layer: {model.input_emb.weight.shape}\")\n",
    "print(f\"Shape Output Layer: {model.out_head.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a97790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters in model: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of trainable parameters in model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ffcbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters in gpt2: 124,412,160\n",
      "\n",
      "Uses weight tying\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Total number of trainable parameters in gpt2: {total_params_gpt2:,}\\n\\nUses weight tying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a547ade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb0ca9",
   "metadata": {},
   "source": [
    "# Lecture 25: Next word prediction from logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac6d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdims=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cae0fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "start_contaext = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_contaext)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "print(f\"encoded: {encoded}\")\n",
    "print(f\"encoded_tensor shape: {encoded_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6060f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]]) 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(out, out.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e559b823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am Featureiman Byeswickattribute argue'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "decoded_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_built_steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

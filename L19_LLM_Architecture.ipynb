{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06468898",
   "metadata": {},
   "source": [
    "# Lecture 19: LLM-Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3278d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fe2ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864baef9",
   "metadata": {},
   "source": [
    "## Building GPT-Model basic structure Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4624340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout_embedding = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias= False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_embeds = self.token_embedding(in_idx)\n",
    "        positional_embeds = self.positional_embedding(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = token_embeds + positional_embeds\n",
    "        x = self.dropout_embedding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8595b5",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2aefef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7312f71",
   "metadata": {},
   "source": [
    "## Step 2: Create an instance of the GPT-Dummy-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "322f3beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model.forward(batch)\n",
    "print(f\"Output shape:\\n{logits}\\n{logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39922eb1",
   "metadata": {},
   "source": [
    "## *interpretation of tensor shape*\n",
    "### 2 - number of batches\n",
    "### 4 - number of tokens per batch\n",
    "### 50257 - vocabulary size \n",
    "### --> output logits hold the propabilities over all the words in the vocabulary, the position of the highest value represents the token from the vocabulary which is most likely to be the next token in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e98f19",
   "metadata": {},
   "source": [
    "## each row within one sub block represents the result of one prediction task\n",
    "### 1st input -> every ---> predicting the word \"effort\" (1st row in sub block)\n",
    "### 2nd input -> every effort ---> predicting the word \"moves\" (2nd row in sub block)\n",
    "### 3rd input -> every effort moves ---> predicting the word \"you\" (3rd row in sub block)\n",
    "### 4th input -> every effort moves you ---> predicting the word \"forward\" (4th row in the sub block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_built_steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97923e4b",
   "metadata": {},
   "source": [
    "# Lecture 16: Causal Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9c873",
   "metadata": {},
   "source": [
    "### only previous tokens and current token are being taken under consideration during computation of the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "9736cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "inputs = torch.tensor([[0.43, 0.15, 0.89],\n",
    "                       [0.55, 0.87, 0.66],\n",
    "                       [0.57, 0.85, 0.64],\n",
    "                       [0.22, 0.58, 0.33],\n",
    "                       [0.77, 0.25, 0.10],\n",
    "                       [0.05, 0.80, 0.55]]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24993c3f",
   "metadata": {},
   "source": [
    "### initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "a30017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = inputs.shape[-1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bac4f2",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "### --> attention weights are going to be computed like L15 trainable weights (scaling by the square root of the keys dimension is performed and the softmax normalization also)\n",
    "\n",
    "## Step 2:\n",
    "### --> attention weights above the diagonal of the attention weights matrix are going to be masked out (set to 0)\n",
    "\n",
    "## Step 3:\n",
    "### --> after zeroing the values above the diagonal a re-normalization is being performed to the masked attention weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6dd63",
   "metadata": {},
   "source": [
    "# Finally the masking is applied to the attention scores before dividing by the square root and before applying softmax in order to avoid data leakage as the softmax already involves future tokens when making the rows sum up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9913c9c",
   "metadata": {},
   "source": [
    "### *Self Attention Class using Linear Layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "d7bebf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights_scaled = attn_scores / torch.sqrt(torch.tensor(keys.shape[-1]))\n",
    "        attn_weights = torch.softmax(attn_weights_scaled, dim=-1)\n",
    "\n",
    "        context_matrix = attn_weights @ values\n",
    "        return context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d866e",
   "metadata": {},
   "source": [
    "### *Instance of Self Attention Class with manually computing Class matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "e4213e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "self_attention_v2 = SelfAttentionV2(d_in, d_out)\n",
    "\n",
    "queries = self_attention_v2.W_query(inputs)\n",
    "keys = self_attention_v2.W_key(inputs)\n",
    "values = self_attention_v2.W_value(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights_scaled = attn_scores / torch.sqrt(torch.tensor(keys.shape[-1]))\n",
    "attn_weights = torch.softmax(attn_weights_scaled, dim=-1)\n",
    "\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0a49e",
   "metadata": {},
   "source": [
    "### generating the masking matrix --> lower triangular matrix, all elements above the diagonal are set to 0\n",
    "\n",
    "### context length specifying the amount of words in the sequence which is represented by the number of rows in the input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "5b69303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Matrix with ones only:\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Apply the tril function to generate triangular Matrix:\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_weights.shape[0]\n",
    "\n",
    "mask_step1 = torch.ones(context_length, context_length)\n",
    "mask_step2 = torch.tril(mask_step1)\n",
    "print(f\"Create Matrix with ones only:\\n{mask_step1}\\n\")\n",
    "print(f\"Apply the tril function to generate triangular Matrix:\\n{mask_step2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b89ea8",
   "metadata": {},
   "source": [
    "### do step1 and step2 in one line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "954d970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9ea40",
   "metadata": {},
   "source": [
    "### multiplying the attention weights matrix with the mask simple matrix to apply the masking on the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "dafcc1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weights = attn_weights * mask_simple\n",
    "print(masked_attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b82315",
   "metadata": {},
   "source": [
    "### normalization of the lower triangular matrix using the sums of the remaining rows and dividing each remaining row value by the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "3b84cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aquiring the remaining sum of each row in the masked matrix:\n",
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "\n",
      "deviding the masked attention weights matrix by the sum of its rows:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sum_rows = masked_attn_weights.sum(dim=1, keepdim=True)\n",
    "masked_attn_weights_norm = masked_attn_weights / sum_rows\n",
    "print(f\"aquiring the remaining sum of each row in the masked matrix:\\n{sum_rows}\\n\")\n",
    "print(f\"deviding the masked attention weights matrix by the sum of its rows:\\n{masked_attn_weights_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53181c7b",
   "metadata": {},
   "source": [
    "### example calculation for the first two rows of the masked attention weights matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "20adf8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example calculation for rows 2 and 3\n",
      "0.5516, 0.4484\n",
      "0.3801, 0.3097, 0.3102\n"
     ]
    }
   ],
   "source": [
    "print(\"Example calculation for rows 2 and 3\")\n",
    "r2_c1, r2_c2 = 0.2041 / 0.37, 0.1659 / 0.37\n",
    "r3_c1, r3_c2, r3_c3 = 0.2036 / 0.5357, 0.1659 / 0.5357, 0.1662 / 0.5357\n",
    "print(f\"{r2_c1:.4f}, {r2_c2:.4f}\")\n",
    "print(f\"{r3_c1:.4f}, {r3_c2:.4f}, {r3_c3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12b5c3",
   "metadata": {},
   "source": [
    "# Approach to prevent data leakage applying triangular mask to attention scores matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdac568",
   "metadata": {},
   "source": [
    "### taking the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "93a7edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
       "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
       "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
       "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
       "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
       "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f698f",
   "metadata": {},
   "source": [
    "### creating upper triangular matrix with ones, essentially the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "c14467c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d737b",
   "metadata": {},
   "source": [
    "### applying the mask to the attention scores and replacing the ones with negativ infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "6bd0b1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attn_scores_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff382c",
   "metadata": {},
   "source": [
    "### dividing by the square root of the keys dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "2df2a86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2050,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.3293, 0.1218,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.3249, 0.1204, 0.1224,   -inf,   -inf,   -inf],\n",
       "        [0.1868, 0.0724, 0.0733, 0.0132,   -inf,   -inf],\n",
       "        [0.1544, 0.0618, 0.0624, 0.0125, 0.0556,   -inf],\n",
       "        [0.2410, 0.0898, 0.0912, 0.0140, 0.0912, 0.0055]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_masked_scaled = attn_scores_masked / torch.sqrt(torch.tensor(keys.shape[-1]))\n",
    "attn_scores_masked_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc9f74",
   "metadata": {},
   "source": [
    "### applying softmax function to normalize the along the rows, does NOT work when 0s are above the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "aa16955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_masked = torch.softmax(attn_scores_masked_scaled, dim=1)\n",
    "attn_weights_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cd4d2",
   "metadata": {},
   "source": [
    "### applying dropout within the causal attention mechanism, when weights are swithed of randomly the remaining weights are going to be rescaled by the dropout rate, hence the 2s in the matrix after dropout is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "5fc7654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 2., 2., 2., 2.],\n",
      "        [2., 0., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 2., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [0., 0., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(0.5)\n",
    "test_case = torch.ones(6, 6)\n",
    "print(dropout(test_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae8be3",
   "metadata": {},
   "source": [
    "### applying dropout to the attention weights matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "7f8a21d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.4925, 0.4638, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.3968, 0.3775, 0.3941, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3058]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_do = dropout(attn_weights_masked)\n",
    "attn_weights_do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324c6ae",
   "metadata": {},
   "source": [
    "### computing the context matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "84e64a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [-0.0925,  0.0445],\n",
       "        [-0.0199,  0.1769],\n",
       "        [-0.0637, -0.0473]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix = attn_weights_do @ values\n",
    "context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ee393",
   "metadata": {},
   "source": [
    "### creating a batch of multiple inputs to do simultaneous processing of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "5ab69baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d8487",
   "metadata": {},
   "source": [
    "# Creating a Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "180f6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens = x.shape[1]\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores_masked = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_scores_masked_scaled = attn_scores_masked / torch.sqrt(torch.tensor(keys.shape[-1]))\n",
    "        attn_weights = torch.softmax(attn_scores_masked_scaled, dim=-1)\n",
    "        attn_weights_do = self.dropout(attn_weights)\n",
    "        context_matrix = attn_weights_do @ values\n",
    "        return context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32dd039",
   "metadata": {},
   "source": [
    "### --> instead of self.mask, self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) can be used which is essentially device agnostic code ensuring the triangular matrix is moved to the correct device automatically\n",
    "\n",
    "### --> keys.transpose has to now do the transpose considering two inout batches which is why the expression changes comared to before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060a7a8",
   "metadata": {},
   "source": [
    "### creating an instance of the causal self attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "352d4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "causal_attention_v1 = CausalAttentionV1(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = causal_attention_v1.forward(batch)\n",
    "print(context_vecs.shape)\n",
    "print(context_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_built_steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
